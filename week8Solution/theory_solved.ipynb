{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: AdaBoost and VC-dimension\n",
    "In this exericse, we will investigate the VC-dimension of voting classifiers and argue that one has to be careful with the number of iterations of boosting to guarantee good generalization performance with high probability.\n",
    "\n",
    "The data we will consider is one-dimensional, i.e. a single feature. Our weak learner is decision stumps, i.e. a base hypothesis $h$ can be specified by a splitting value $v$, and two leaf values $a, b \\in \\{-1,1\\}$. Given a one-dimensional feature vector $x = (x_1)$, the hypothesis $h$ returns $a$ when $x_1 \\leq v$ and it returns $b$ otherwise. We let $H$ denote the set of all such decision stumps, i.e. $H$ is the set of base hypotheses.\n",
    "\n",
    "### VC-dimension of base classifier\n",
    "1. Show that the VC-dimension of $H$ is $2$.\n",
    "\n",
    "### VC-dimension of voting classifiers\n",
    "Let $C$ be the class of all voting classifiers over $H$, i.e. $C$ consists of all hypotheses of the form:\n",
    "\n",
    "$$\n",
    "f(x) = \\textrm{sign}(\\sum_{h \\in H} \\alpha_h h(x))\n",
    "$$\n",
    "with $\\sum_{h \\in H} \\alpha_h = 1$ and $\\alpha_h \\geq 0$ for all $h$. We will show that $C$ has infinite VC-dimension. To do so, we will argue that for any size $n$, there is a set of $n$ points $x_1,\\dots,x_n$ in 1d that can be shattered, i.e. for any labeling $y \\in \\{-1,1\\}^n$ of the $n$ points, there is a hypothesis $g$ in $C$ such that $g(x_i)=y_i$ for all $i$. \n",
    "\n",
    "We will consider the simple point set $x_i = i$, i.e. $n$ evenly spaced points on the line. Let $y \\in \\{-1,1\\}^n$ be any labeling of the $n$ points. We need to show that there is a hypothesis $g \\in C$ that has $h(x_i)=y_i$ for all $i$. To do so, we will use the fact from the lecture that if our base learner guarantees that there is some $\\gamma > 0$, such that for every probability distribution $D$ over $x_1,\\dots,x_n$, the base learner produces a hypothesis $h \\in H$ such that the weighted $0-1$ loss satisfies $L_D(h) = \\sum_{i=1}^n D(i)1_{h(x_i) \\neq y_i} \\leq 1/2-\\gamma$, then running AdaBoost for $(\\ln n)/(2 \\gamma^2)$ iterations produces a voting classifier $f$ with $0$ error on $x_1,\\dots,x_n$.\n",
    "1. Assume we have some probability distribution $D$ over $x_1,\\dots,x_n$ and assume that there is a hypothesis $h \\in H$ with $L_D(h) \\leq 1/2-\\gamma$. Show how to modify the algorithm from the Decision Tree lecture that computes decision stumps, such that it is guaranteed to output a hypothesis $h'$ with $L_D(h') \\leq 1/2-\\gamma$.\n",
    "2. Argue that for any probability distribution $D$ over $x_1,\\dots,x_n$, there is a hypothesis $h \\in H$ with $L_D(h) \\leq 1/2 - 1/(2n)$. Hint: Consider hypotheses where the splitting point is right before and right after the point $x_i$ with largest probability $D(i)$.\n",
    "3. Use the above to conclude that there is a hypothesis $g \\in C$ with $g(x_i) = y_i$ for all $i$.\n",
    "\n",
    "### SOLUTION MATH HERE\n",
    "We start with the base learner question:\n",
    " 1. VC dimension of $H$. First consider two points in 1d with coordinated 1 and 2 respectively. For any labeling, we can create the decision stump that splits between them and assigns the correct labeling in each leaf. So VC dimension at least 2. For three points $x_1 \\leq x_2 \\leq x_3$, observe that it is impossible to construct the labeling $(-1,1,-1)$, so VC-dimension at most 2.\n",
    " \n",
    " \n",
    "Next let us consider the voting classifier question: \n",
    " 1. Given a probability distribution $D$, try all $n-1$ relevant splits at the root. For each split, return in each leaf the label that minimizes the weighted $0-1$ loss on the subset of points in that leaf. This algorithm explicitly compares all possible decision stumps so it is guaranteed to find the optimal decision stump. That stump $h'$ must have $L_D(h') \\leq 1/2-\\gamma$ since it is the optimal stump.\n",
    " 2. We will argue that there is a hypothesis $h$ that splits either right before or right after $x_i$ that achieves $L_D(h) \\leq 1/2 - 1/(2n)$. For this, ignore $x_i$ for now and consider any hypothesis $h$ that for all $x_j$ with $j<i$ returns a value $a \\in \\{-1,1\\}$ minimizing the weighted 0-1 loss on those points $x_j$, and for any $x_j$ with $j >i$ returns a value $b \\in \\{-1,1\\}$ mininizing the weighted 0-1 loss on those points. We must have $\\sum_{j<i}D(j)1_{h(x_j) \\neq y_j} \\leq (1/2)\\sum_{j<i}D(j)$ and $\\sum_{j>i}D(j)1_{h(x_j) \\neq y_j} \\leq (1/2)\\sum_{j>i}D(j)$. Let us denote the advantages over guessing by $\\gamma_< = (1/2)\\sum_{j<i}D(j)-\\sum_{j<i}D(j)1_{h(x_j) \\neq y_j} \\geq 0$ and $\\gamma_> = (1/2)\\sum_{j>i}D(j) - \\sum_{j>i}D(j)1_{h(x_j) \\neq y_j} \\geq 0$. We now have two cases: a) At least one of $\\gamma_<$ and $\\gamma_>$ is greater than $D(i)/2$. In this case, consider splitting such that $x_i$ is placed on the side of the split with the smallest advantage. We can then alter the prediction on that side to still guarantee an error on that side of at most 1/2 the sum of $D(j)$'s (including summing over $D(i)$). The other side still has an advantage of at least $D(i)/2$ so the final advantage is at least $D(i)/2$. b) Both $\\gamma_<$ and $\\gamma_>$ are smaller than $D(i)/2$. Consider flipping the prediction of one side. Then the \"advantage\" is at least $-D(i)/2$. We can thus add $x_i$ to any side and make that side predict $y_i$, yielding an advantage of at at least $D(i)-D(i)/2 = D(i)/2$. Since $D(i) \\geq 1/n$, we are done.\n",
    " 3. We know AdaBoost produces a classifier with $0$ error after $(\\ln n)/(2 (1/(2n))^2) = (2 \\ln n)n^2$ iterations, hence there must be a voting classifier in $C$ that has $g(x_i)=y_i$ for all $i$.\n",
    " \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implementing AdaBoost\n",
    "In this exercise your task is to implement AdaBoost as described in the lecture and the Boosting note.\n",
    "We have provided starter code in adaboost.py. See the boosting note for a description of the algorithm.\n",
    "\n",
    "You must implement the methods\n",
    "- ensemble_output\n",
    "- exp_loss\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order\n",
    "\n",
    "To test your implementation, run adaboost.py\n",
    "\n",
    "You should get a final accuracy of around 0.886\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Boosting by Hand\n",
    "In this exercise you must complete one step of gradient boosting with exponential loss on a small data set (X, y) as shown below. The exponential loss is $L(h(x),y) = e^{-yh(x)}$.\n",
    "\n",
    "$X = [1,2,5,3,4]$\n",
    "\n",
    "$y = [1,1,1,-1, -1]$\n",
    "\n",
    "Assume that we initialize our ensemble model with the constant function $h(x) = 1$\n",
    "\n",
    "\n",
    "**Your task requires the following three steps** \n",
    "1. Compute the gradients the regression tree should fit (with least squares)\n",
    "2. Construct the Regression Stump found by fitting the negative gradient\n",
    "3. Optimize the leaf values such that the newly added tree minimize the exponential loss, with the condition that the number the leaf returns is in the interval [-1, 1].\n",
    "   What happens if we do not have this constraint that the leaf must return values in [-1, 1]?\n",
    "\n",
    "### SOLUTION MATH HERE\n",
    "The loss function is exp(-y h(x)). Computing gradient wrt. h(x) gives -y * exp(-y h(x)). Evaluating this at 1 and -1 gives the following gradients using that h(x)=1 for all x: (-1/e, -1/e, -1/e, e, e). The residuals are thus (1/e,1/e, 1/e,-e,-e).\n",
    "\n",
    "We consider all non-trivial splits for the root:\n",
    "x<=1: One node with the single element -1/e, one node with -1/e, -1/e, e, e. Cost of first leaf is 0. Second leaf has mean (-1/e + e)/2. Cost is thus 2*(-1/e - (-1/e+e)/2)^2 + 2*(e-(-1/e+e)/2)^2 = 4*((e+1/e)/2)^2 = 9.52\n",
    "\n",
    "x<=2: One node with -1/e, -1/e, one node with -1/e, e, e. Cost of first is 0. Second leaf has mean (-1/e + 2e)/3. Cost is thus (-1/e - (-1/e + 2e)/3)^2 + 2*(e - (-1/e + 2e)/3)^2 = 6.35\n",
    "\n",
    "x<=3: One node with -1/e, -1/e, e and one with -1/e, e. First has mean (-2/e + e)/3. Second has mean (-1/e+e)/2. Cost of first leaf is 2*(-1/e - (-2/e+e)/3)^2 + (e-(-2/e+e)/3)^2 = 6.35. Cost of second leaf is 2*((e-1/e)/2)^2 = 2.76. Cost bigger than x<=2.\n",
    "\n",
    "x<=4: One node with -1/e, -1/e, e, e and one with -1/e. Cost is equal to split x<=1.\n",
    "\n",
    "Best split is x<=2.\n",
    "\n",
    "First leaf has two elements with label 1 and already predicting 1, hence best return value in the range is 1 (minimizing 2*exp(-1*(1+v)). For the second leaf, we have elements with labels 1, -1, -1 and current prediction 1. If predict v in this leaf, then exponential loss on these become exp(-(1 * (1 + v)) + 2*exp(-(-1 * (1 + v))) = e^(-v) * 1/e + e^v * 2e. The derivative is -1/e * e^(-v) + 2e * e^v. Setting to 0: 2e * e^v = 1/e * e^(-v) <=> v + ln(2e) = -v + ln(1/e) <=> v = (1/2)ln(1/(2e^2)) = -1.35.\n",
    "\n",
    "So to optimize the exponential loss, we predict -1 in the newly created leaf.\n",
    "\n",
    "If we did not have the constraint that leaves must output values in $[-1,1]$, then the first leaf, which was minimizing $2*\\exp(-1(1+v))$ would let $v = \\infty$.\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implementing Gradient Boosting\n",
    "In this exercise your task is to implement the Gradient Boosting algorithm for regression using Least Squares Loss.\n",
    "We have provided starter code in **gradientboost.py**. \n",
    "\n",
    "You must implement the methods\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order.\n",
    "\n",
    "\n",
    "Notice that fit gets two sets of data and labels X, y and X_val, y_val.\n",
    "The latter X_val, y_val is a separate validation test set you must test your current ensemble on in each iteration so we can plot the development on data not used for training.\n",
    "\n",
    "To test your implementation, run gradientboost.py -max_depth 1\n",
    "\n",
    "You can provide different max_depth of the base learner which is a Regression Tree (1 is default).\n",
    "\n",
    "With a default base learner with max depth 1 the mean least squares error on both training and test data should be around 0.35. \n",
    "If you change random state then the results may be different.\n",
    "\n",
    "If you increase the max_depth the results will change.  Try for instance max_depth 3 and 5 as well. What do you see?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
