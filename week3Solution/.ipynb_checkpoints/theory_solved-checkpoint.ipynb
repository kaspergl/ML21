{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises week 3\n",
    "**Like last week, it is very imporant that you try to solve every exercise. \n",
    "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "Exercise 6 in particular, is pretty technical so do despair if you cannot solve it.\n",
    "\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generalization\n",
    "\n",
    "\n",
    "## Questions:\n",
    "The Hoeffding bound gives us the following guarantee:\n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 2Me^{-2\\varepsilon^2 n},\n",
    "$$\n",
    "where the probability is over the random choice of the sample.\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "\n",
    "### solution math\n",
    "No since $M$ is infinite\n",
    "### end solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix of Derivatives\n",
    "\n",
    "In Linear Regression we define the in sample error as (ignoring the normalizing factor 1/n)\n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n",
    "\n",
    "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n",
    "\n",
    "$$X=\\begin{pmatrix}\n",
    "- & x_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n",
    "\n",
    "The in-sample error rate $E_{in}$ is then equal to \n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 = \\|Xw-y\\|^2 = (Xw-y)^\\intercal (Xw-y)$$\n",
    "\n",
    "\n",
    "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin}=(X^\\intercal X)^{-1} X^\\intercal y$. \n",
    "\n",
    "To do this we used facts about the derivatives. \n",
    "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is of size $b\\times a$ where\n",
    "$$ \\left[\\frac{\\partial f}{\\partial z} \\right]_{i,j} = \\frac{\\partial f_i}{\\partial z_j} $$\n",
    "\n",
    "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1, x_2]) = [x_1, x_2, x_1 \\cdot x_2]$ then \n",
    "the matrix of derivatives has shape $3 \\times 2$ and looks like $ \\frac{\\partial f}{\\partial x} =\n",
    " \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1  \\\\\n",
    "  x_2  & x_1 \\\\\n",
    " \\end{bmatrix} $\n",
    "\n",
    "In our proof we used the following identities about the matrix of derivatives\n",
    "\n",
    "* $f: R^d \\rightarrow R^d, f(z) = Xz-y$, the matrix of derivatives $\\frac{\\partial f}{\\partial x}$ is $X$\n",
    "* $g: R^d \\rightarrow R, g(z) = z^\\intercal z$, the matrix of derivatives $\\frac{\\partial g}{\\partial x}$ is $2z^\\intercal$\n",
    "\n",
    "Notice that $E_\\textrm{in} = g(f(w))$. With these identities, we compute the gradient $\\nabla E_\\text{in}$ by multiplying the matrix of derivativers of $g$, and $f$ evaluated at their inputs (abiding the mighty Chain Ruler), and take the transpose. https://en.wikipedia.org/wiki/Chain_rule\n",
    "\n",
    "Since $f(w) = Xw- z$, $g(f(w)) = (Xw-y)^\\intercal (Xw - y)$\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\nabla E_\\textrm{in} = (2(Xw-y)^\\intercal I X)^\\intercal = 2X^\\intercal(Xw- y)\n",
    "$$\n",
    "\n",
    "Solving for the zero vector gives $w_\\textrm{lin}$ (and the 2 factor becomes irrelevant, like the initial 1/n scaling).\n",
    "\n",
    "## Your job is to prove the two identities. \n",
    "\n",
    "* Let $f(w) = Xw - y$, Where $X$ is a $n \\times d$ matrix, $y$ is a $n\\times 1$ vector and $w$ is a $w \\times 1$ vector (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^n$). \n",
    "Show that the matrix of derivatives of $f$ is X. \n",
    "\n",
    "Hint: You can think of $f = [f_1,\\dots,f_n]$ as n output functions where $f_i(w) = x_{i}^\\intercal w - y_i$ and $x_i$ is the i'th row of $X$ (as a column vector) and $y_i$ is the i'th entry in vector $y$. Start with $\\frac{\\partial f_1}{\\partial w_1}$ to see if a pattern emerges\n",
    "* Let $g(z) = z^\\intercal z$ where z is a vector (the squared norm of $z$). Show that the matrix of derivatives is $2z^\\intercal$\n",
    "\n",
    "\n",
    "### solution math\n",
    "We have\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial w_j} = x_{i,j}\n",
    "$$\n",
    "And thus by definition, we get that the matrix of derivates equals $X$.\n",
    "\n",
    "Next observe that\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial z_j} = \\sum_i \\frac{\\partial z_i^2}{\\partial z_j} = 2z_j\n",
    "$$\n",
    "and the identity follows.\n",
    "### end solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression and the missing inverse\n",
    "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$, is found simply by computing the matrix product\n",
    "$$\n",
    "(X^\\intercal X)^{-1}X^\\intercal y\n",
    "$$\n",
    "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n",
    "\n",
    "In class i suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n",
    "In this exercise you must argue that this is a good idea.\n",
    "\n",
    "To do this, you must prove/argue the two following two things\n",
    "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n",
    "* If $(X^\\intercal X)$ is not invertible then $X$ contains linear dependent columns\n",
    "\n",
    "\n",
    "### solution math\n",
    "Let $X$ be $n \\times d$. $(X^\\intercal X)$ a $d \\times d$ matrix. If it is not invertible, then its rank is less than $d$. Since rank($(X^\\intercal X)$)=rank($X$) and rank($X$) is equal to the rank of the column space of $X$, it follows that the column space of $X$ must have rank less than $d$. Hence there must exist a set of linearly dependent columns. \n",
    "\n",
    "To see that the cost of the optimal solution does not change when removing columns, let $X'$ be the matrix obtained by removing one column of $X$ which is linearly dependent of some subset of the remaining columns in $X'$. Since $Xw$ is in the span of the columns of $X$ for any vector $w$, it must be the case that $Xw$ is also in the span of the columns of $X'$ for every $w$. Hence for any vector $w$, there is a vector $w'$ such that $\\|X'w'-y\\|_2^2 = \\|Xw-y\\|_2^2$. Thus we only need to argue that there cannot exists a vector $w'$ with $\\|X'w'-y\\|_2^2 < \\|Xw-y\\|_2^2$ for all $w$. But this is trivially true as we can pad $w'$ with zeroes and obtain $w''$ such that $Xw''=X'w'$.\n",
    "Maximizing \n",
    "\n",
    "### end solution\n",
    "\n",
    "\n",
    "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n",
    "\n",
    "HINT 2: $Xw$ is in the column space of $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Maximum Likelihood Linear Regression\n",
    "\n",
    "The Linear Regression method may also be derived as a maximum likelihood\n",
    "procedure. In linear regression the function we learn is choosen to\n",
    "minimize mean squared error, a criterion that we introduced more or less\n",
    "arbitrarily.\n",
    "\n",
    "I.e. Given X, y compute\n",
    "$$\n",
    "w_{\\textrm{opt}} = \\textrm{argmin}_w: \\sum_{i=1}^n (w^\\intercal x_i - y_i)^2\n",
    "$$\n",
    "We now revisit Linear Regression from the point of view of maximum\n",
    "likelihood estimation. \n",
    "\n",
    "We consider the target function a conditional distribution $p(y\n",
    "| x)$ and assume it is defined as\n",
    "$$\n",
    "p(y\\mid x,w) = w^\\intercal x + \\varepsilon, \n",
    "$$ \n",
    "for some unknown $w$, where $\\varepsilon$ is a\n",
    "noise term independent of $x$ that is normally distributed with zero\n",
    "mean and variance $\\sigma^2$ i.e. \n",
    "$$\n",
    "\\mathbb{E}[\\varepsilon] = 0, \\mathbb{E}[\\varepsilon^2] =\\sigma^2\n",
    "$$\n",
    "\n",
    "For the (1D) normal distribution with mean $\\mu$ and variance $\\sigma^2$\n",
    "the probability density function is\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n",
    "$$\n",
    "\n",
    "In other words, given $x$, the target function  outputs a value $y$ that is\n",
    "distributed as a Gaussian (normal distribution) around $w^\\intercal\n",
    "x$. \n",
    "We can now write $p(y\\mid x,w)$ as\n",
    "$$\n",
    "p(y \\mid x,w ) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(y-w^\\intercal x)^2/2\\sigma^2}\n",
    "$$\n",
    "for some unknown $w$ that we wish to learn. \n",
    "\n",
    "\n",
    "We want to make an algorithm that computes the maximum likelihood\n",
    "parameters $w_\\textrm{ml}$ of our model. We are given a data set $D\n",
    "= \\{(x_i, y_i) \\mid i = 1, \\dots, n\\}$ and for a fixed $w$ we let $P(D\n",
    "\\mid w) = \\prod_{i=1}^n p(y_i \\mid x_i ,w)$ be the likelihood of the\n",
    "data given $w$. Your job is to derive an algorithm for computing the\n",
    "maximum likelihood parameters, namely\n",
    "$w_\\mathrm{ml} = \\operatorname*{arg\\,max}_w\n",
    "P(D \\mid w)$.\n",
    "\n",
    "### solution math\n",
    "$$\n",
    "\\prod_{i=1}^n p(y_i \\mid x_i ,w)\n",
    "$$\n",
    "is the same as minimizing\n",
    "$$\n",
    "-\\ln\\left( \\prod_{i=1}^n p(y_i \\mid x_i ,w)\\right) = \\sum_i -\\ln(p(y_i \\mid x_i ,w))\n",
    "$$\n",
    "Inserting $p(y_i \\mid x_i,w)$, we get\n",
    "$$\n",
    "\\sum_i (y_i-w^\\intercal x_i)^2/2\\sigma^2 + (1/2)\\ln(2 \\sigma^2 \\pi)\n",
    "$$\n",
    "We can ignore all constant factors and additive constants when minimizing this expression and get that we should minimize\n",
    "$$\n",
    "\\sum_i (y_i-w^\\intercal x_i)^2 = \\|Xw-y\\|^2\n",
    "$$\n",
    "### end solution\n",
    "\n",
    "**Hint:** Minimize the negative log likelihood of the data instead and note\n",
    "that we end up with a formula for computing $w$ that should look\n",
    "familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
