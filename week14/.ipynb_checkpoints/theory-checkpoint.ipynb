{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: Basic Text Classification\n",
    "In this exercise you must implement a basic text classification pipeline and apply it to SMS spam classification.\n",
    "The pipeline is\n",
    "\n",
    "* Clean strings and split them into words.\n",
    "    You need to complete the implementation of **standardize_strings**, **split_strings**.\n",
    "* Make vocabulary/dictionary with mappings from word to index and index to word and transforming vectors.\n",
    "    You need to complete the implementation of **make_vocabulary_and_index**, **words_to_vectors**\n",
    "* Apply a standard Logistic Regression classifier on given data and output the results.\n",
    "  You need to complete the implementation of **run_bow_classifier**\n",
    "\n",
    "There is a lot of code and hopefully helpful comments.\n",
    "\n",
    "If the technicalities of this becomes to hard you can move on to the next exercise where we use a standard implementation from sklearn in python to steps 1 and 2 as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    #wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "    with open('smsspamcollection/SMSSpamCollection', 'r') as f:\n",
    "        dat = f.readlines()\n",
    "    labels = [x.split('\\t')[0] for x in dat]\n",
    "    texts = [x.split('\\t')[1] for x in dat]\n",
    "    sl = set(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def test_vectorize():\n",
    "    \"\"\" \n",
    "    test words_to_vectors \n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxz'\n",
    "    vocab = {x for x in alphabet}\n",
    "    index_map = {x: i for (i, x) in enumerate(sorted(vocab))}\n",
    "    a = [1, 2]\n",
    "    b = [5]\n",
    "    c = [0, 10]\n",
    "    index_lists = [a, b, c]\n",
    "    word_lists = [[alphabet[i] for i in x] for x in index_lists]\n",
    "    \n",
    "    print(\"TESTING 'make_vocabulary_and_index': \\t\\t\", end='')\n",
    "    learned_vocab, word_to_index, index_to_word = make_vocabulary_and_index(word_lists)\n",
    "    target_vocab = {'a', 'b', 'c', 'f', 'k'}    \n",
    "    \n",
    "    assert target_vocab == learned_vocab, \"Error with 'make_vocabulary_and_index'.\\nExpected vocabulary: \\t{0}\\n Got: \\t\\t{1}\".format(target_vocab, learned_vocab)\n",
    "    # Test word_to_index and index_to_word by checking they are inverse mappings. \n",
    "    # inv_map = {v: k for k, v in tc.word_to_index.items()} \n",
    "    # assert tc.index_to_word == inv_map, \"Error with 'make_vocabulary_and_index'.\\nExpected that the dictionaries 'index_to_word' and 'word_to_index' are each others inverse. This was not the case. \"\n",
    "    print(\"PASSED!\")\n",
    "\n",
    "    print(\"TESTING 'words_to_vector': \\t\\t\\t\", end='')\n",
    "    string_vectors = words_to_vectors(word_lists, vocabulary=vocab, index_map=word_to_index)\n",
    "    def my_idx(sid, wid):\n",
    "        return word_to_index[word_lists[sid][wid]]\n",
    "    target = np.zeros((3, len(vocab)))\n",
    "    target[0, my_idx(0, 0)] = 1\n",
    "    target[0, my_idx(0, 1)] = 1\n",
    "    target[1, my_idx(1, 0)] = 1\n",
    "    target[2, my_idx(2, 0)] = 1\n",
    "    target[2, my_idx(2, 1)] = 1\n",
    "    assert np.allclose(string_vectors, target),  \"Error with words to vectors\"\n",
    "    \n",
    "    # print(string_vectors-target)\n",
    "    print('PASSED!')\n",
    "\n",
    "def test_string_cleaning():\n",
    "    \"\"\" \n",
    "    Test string cleaning\n",
    "    \"\"\"\n",
    "    strings = ['i THINK machine LEARNING is cool.,-()', 'he likes cake cake cake',\n",
    "               'pandora has a box that should not be opened']\n",
    "\n",
    "    bad_words = {'i', 'the', 'has', 'he', 'has', 'a', 'is'}\n",
    "    result = [['think', 'machine', 'learning', 'cool'], ['likes', 'cake', 'cake', 'cake'],\n",
    "              ['pandora', 'box', 'that', 'should', 'not', 'be', 'opened']]\n",
    "\n",
    "    print(\"TESTING 'standardize_strings': \\t\\t\\t\", end='')\n",
    "    expect = ['i think machine learning is cool', 'he likes cake cake cake', 'pandora has a box that should not be opened']\n",
    "    strings_cleaned = standardize_strings(strings) # lower string remove special characters\n",
    "    assert strings_cleaned == expect, \"Error in 'standardize_strings'.\\nExpected \\t{0}\\nGot \\t\\t{1}\".format(expect, strings_cleaned)\n",
    "    print('PASSED!')\n",
    "\n",
    "    print(\"TESTING 'split_strings': \\t\\t\\t\", end='')\n",
    "    expect = [['i', 'think', 'machine', 'learning', 'is', 'cool'], ['he', 'likes', 'cake', 'cake', 'cake'], ['pandora', 'has', 'a', 'box', 'that', 'should', 'not', 'be', 'opened']]\n",
    "    word_lists = split_strings(strings_cleaned)\n",
    "    assert word_lists == expect, \"Error in 'split_strings': \\nExpected\\t{0}\\nGot \\t\\t{1}\".format(expect, word_lists)\n",
    "    print(\"PASSED!\")\n",
    "    \n",
    "    \n",
    "def standardize_strings(string_list):\n",
    "    \"\"\" \n",
    "    Standardize strings by \n",
    "        1. making them lower case (example 'LaTeX' -> 'latex')\n",
    "        2. remove any non-alphabetic characters, i.e. \n",
    "           characters not in [a-z]. (example \"l33t_h@x\" -> \"lthx\")\n",
    "\n",
    "    For example:\n",
    "        >>> standardize_strings(['remove . please', 'also .,-_()'])\n",
    "        ['remove  please', 'also ']\n",
    "    \n",
    "    The following methods might be useful: \n",
    "        str.lower,\n",
    "        re.sub (regular expression from re package), \n",
    "        str.replace, \n",
    "\n",
    "\n",
    "    You can ignore irrelevant spacing since we'll take care of it later. \n",
    "\n",
    "    Args:\n",
    "    string_list: list of strings\n",
    "    \n",
    "    Returns:\n",
    "    list of strings without non-alphabetic characters\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    ### YOUR CODE 1-3 lines\n",
    "    ### END CODE\n",
    "    \n",
    "    assert len(string_list) == len(res)\n",
    "    return res\n",
    "\n",
    "def split_strings(strings):\n",
    "    \"\"\" \n",
    "    Split a list of strings into list of list of words. The splitting should be done on space ' '.\n",
    "\n",
    "    For example: \n",
    "        >>> split_strings(['split me please', 'me to'])\n",
    "        [['split', 'me', 'please'], ['me', 'to']]\n",
    "        \n",
    "    Try to use list comprehension instead of writing function with loops or recursion. \n",
    "    List comprehension works as follows\n",
    "\n",
    "        >>> dat = list(range(6)) \n",
    "        >>> dat\n",
    "        [0,1,2,3,4,5]\n",
    "\n",
    "        >>> dat_squared = [x**2 for x in dat] \n",
    "        >>> dat_squared\n",
    "        [0, 1, 4, 9, 16, 25]\n",
    "\n",
    "    The following method might be useful:\n",
    "        - str.split()  (for help with splitting strings)\n",
    "\n",
    "    Args:\n",
    "    strings: list of strings\n",
    "\n",
    "    Returns:\n",
    "    list of lists of strings (words)\n",
    "    \"\"\"\n",
    "    word_lists = []\n",
    "    \n",
    "    ### YOUR CODE \n",
    "    ### END CODE\n",
    "    \n",
    "    assert len(word_lists) == len(strings)\n",
    "    return word_lists\n",
    "\n",
    "\n",
    "\n",
    "def make_vocabulary_and_index(word_lists):\n",
    "    \"\"\"\n",
    "    Compute the vocabulary (set), word_to_index (dict), and index_to_word\n",
    "    for the classifier.\n",
    "     - vocabulary must be a set with all words in the list of lists of words word_lists\n",
    "     - word_to_index is a dictionary that maps each word to a unique index\n",
    "     - index_to_word is a dictionary which is the inverse of word_to_index\n",
    "\n",
    "    Use set and dict comprehension  (like list comprehension) to _ each with one list of code\n",
    "    {x for x in [1,2,3,4,5,6]} is a set comprehension which makes a set of elements in [1,2,3,4,5,6]\n",
    "\n",
    "    As an example\n",
    "    If word_lists is [['a', 'b'], ['c','b']] then vocabulary should be {'a', 'b', 'c'} and word_to_index should be something like {'a':0, 'b':1, 'c':2} \n",
    "    (the permutation is irrelevant, this is just the obvious one.) \n",
    "    Also, index_to_word should be in this case {0:'a', 1:'b', 2:'c'}\n",
    "\n",
    "    Args:\n",
    "    word_lists: list of lists of strings\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "\n",
    "    ### YOUR CODE 3-4 lines\n",
    "    ### END CODE\n",
    "\n",
    "    assert len(word_to_index) == len(index_to_word)\n",
    "    return vocabulary, word_to_index, index_to_word\n",
    "    \n",
    "def words_to_vectors(word_lists, vocabulary, index_map):\n",
    "    \"\"\"\n",
    "    Each list of strings in word_lists is turned into a vector of length |vocabulary| in such a way that the ith entry of the vector is the number of occurences of word i (under index_map) in the string.\n",
    "    index_map is the dictionary {word : index}. If the word is not in the vocabulary you should ignore it. \n",
    "\n",
    "    As an example\n",
    "    If the vocabulary is {'a','b','c','d','e'} and the mapping index_map is {'a':0,'b':1,'c':2,'d':3,'e':4}, then the word_list ['a','b','a'] is mapped to [2, 1, 0, 0, 0]\n",
    "\n",
    "    A way of doing this is as follows (you can come up with a different implementation as long as the results are the same):\n",
    "    1. Map each word to its index transforming list of words to list of indices\n",
    "    2. Fill the vectors using these indices\n",
    "\n",
    "    Step 1 can be done easily.\n",
    "    For the second step the collections.Counter class may be useful: given a list it returns a dictionary-like object counting the ocurrences of each entry in the list. \n",
    "    As an example of this:\n",
    "    c = Counter([1,1,2]) # -> Counter({1: 2, 2: 1})\n",
    "    Now c.keys, c.values, c.items gives the list of indices and counts\n",
    "    In [1]: list(c.keys())\n",
    "    Out[1]: [1, 2]\n",
    "    In [2]: list(c.values())\n",
    "    Out[2]: [2, 1]\n",
    "\n",
    "    Remember indexing in numpy:\n",
    "    if x is a numpy array and ind a list of indices, then a[ind] indexes into the entries of x given in ind.\n",
    "\n",
    "    Args:\n",
    "    word_lists: list of lists of strings (each string is in self.vocabulary)\n",
    "\n",
    "    Returns: \n",
    "    word_vectors: numpy array of size |word_lists| X |vocabulary|. The ith row is the vector to which the ith word_list is mapped by counting the occurrences of each word.\n",
    "    \"\"\"\n",
    "    word_vectors = np.zeros((len(word_lists), len(vocabulary)))\n",
    "\n",
    "    ### YOUR CODE\n",
    "    ### END CODE\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def data_clean(strings, stopwords = set()):\n",
    "        \"\"\"\n",
    "        Clean the data by calling the string functions you made\n",
    "        Nothing required here\n",
    "        \"\"\"\n",
    "        clean_data = standardize_strings(strings)\n",
    "        word_lists = split_strings(clean_data)\n",
    "        return word_lists\n",
    "    \n",
    "def run_bow_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test (use same transform for both)\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    print(train_strings[0:10])\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "  \n",
    "test_string_cleaning()\n",
    "test_vectorize()  \n",
    "run_bow_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: TFIDF Using standard tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for a default implementation of transforming text to TFIDF transformed vectors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "Re do the text classification from above using the TfidfVectorizer class.\n",
    "You can play with the parameters if you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def run_tfidf_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test using TFIDF transform\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "\n",
    "\n",
    "run_tfidf_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Analyzing Feature Hashing\n",
    "In feature hashing, we map a vector $x \\in R^d$ to a vector in $R^k$ using two hash functions $h : [d] \\to [k]$ and $g : [d] \\to \\{-1,1\\}$. The hash functions are chosen randomly and independently before seeing any data. Assume the hash functions satisfy the following two properties:\n",
    "1. For any two distinct coordinates $i \\neq j$, we have that $g(i)$ and $g(j)$ are independent and uniform random, i.e. for any $a,b \\in \\{-1,1\\}$ it holds that $\\Pr_g[g(i)=a \\wedge g(j)=b] = 1/4$.\n",
    "2. For any two distinct coordinates $i \\neq j$, we have that $\\Pr_h[h(i)=h(j)] \\leq 1/k$.\n",
    "\n",
    "The embedding $f(x)$ of a vector $x$ is obtained by hashing each index $i \\in [d]$ to the index $h(i)$ and adding $g(i) \\cdot x_i$ to $f(x)_{h(i)}$.\n",
    "\n",
    "Your task it to prove:\n",
    "1. For two vectors $x,y$, we have $\\mathbb{E}[f(x)^\\intercal f(y)] = x^\\intercal y$.\n",
    "\n",
    "Hint: The following re-writing may be useful:\n",
    "$$\n",
    "f(x)^\\intercal f(y) = \\sum_{i=1}^d \\sum_{j=1}^d 1_{[h(i)=h(j)]} x_i y_j g(i) g(j),\n",
    "$$\n",
    "where $1_{[h(i)=h(j)]}$ is the indicator random variable taking the value $1$ if $h(i)=h(j)$ and $0$ otherwise.\n",
    "You may also need linearity of expectation $\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]$ and that for independent random variables $X,Y$ we have $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4: Skip-Gram\n",
    "This exercise can be found in the separate notebook skipgram.ipynb. We recommend uploading the notebook to Google Colab for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
