{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Neural Network Design\n",
    "In this exercise your task is to neural networks by hand that compute simple functions.\n",
    "For the nonlinear transform you can mix them any way you like but you can only use, identity, sign, relu and sigmoid transforms in the neurons.\n",
    "You can make the networks as wide and deep as you would like but small networks are sufficient.\n",
    "* Make a network that computes $c \\cdot x$ for any constant c\n",
    "* Make a network that computes xor of inputs $x_1$ and $x_2$. \n",
    "* Make a network that computes max($x_1$,$x_2$)\n",
    "* Make a network that computes $x^2$ - for x in range {2,3,4,5} i.e. x is an integer\n",
    "\n",
    "\n",
    "- **Hint 1: It is usually easier to find an easy mathematical expression that solves the problem and then to make a network that implements that**\n",
    "- **Hint 2: The only nonlinear transfrom the teacher uses is relu (and identity)**.\n",
    "\n",
    "## SOLUTION MATH HERE\n",
    "A single neuron. Weights c. Nonlinear-transform is identity.\n",
    "\n",
    "In first layer: One neuron with weights (1,1) and bias -1, taking ReLU. Another neuron with weights (-1,-1) bias +1, taking ReLU. Output layer one neuron with weight (-1,-1) and bias 1, identity.\n",
    "\n",
    "In first layer: One neuron with weights (1,-1), taking ReLU. Another neuron with weights (-1,1) taking ReLU. And a neuron computing the sum of the two inputs using weights (1,1), identity. Output layer has weights (1/2,1/2,1/2) and identity. Observe: Sum of first two neurons in first layer is max(x_1,x_2)-min(x_1,x_2). Adding x_1+x_2 cancels out the smallest and leaves twice the max. Dividing it all by 2 gives the desired result.\n",
    "\n",
    "Simpler result by Rasmus and William: two neurons. The first has weights (=1,-1), taking ReLU. The second one has weights (1,0,1), where (1) is from output of the first neuron, and the other (1) is from the variable with weight -1 in the first neuron. Observe that if x_1>x_2, we have x_1 = x_1-x_2 + x_2 = out(first neuron) + input(second neuron). Otherwise x_2 is the answer, with the output of the first neuron being 0.\n",
    "\n",
    "\n",
    "We create a small network testing whether x = y. This is done by having two neurons, one having weight -1 and bias +y and another having weight 1 and bias -y. Both have ReLU. If x=y then both are 0. Otherwise, one is 0 and one is >= 1. Now create a new neuron taking the two previous as inputs with weight -1000 on both and bias y^2, taking ReLU. This becomes y^2 iff x=y, and otherwise it becomes 0. Do this for every y in 2,3,4,5 and create a neuron with weight 1,1,1,1 summing these and using identity.\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Neural Net Forward Pass - Vectorized\n",
    "Implement the score (least squares error), and predict function for a a neural net class for regression.\n",
    "The neural net has one hidden layer with  $\\textrm{relu}(x) = \\max(0, x)$ nonlinearity and one output neuron.\n",
    "\n",
    "For the prediction method you must write an algorithm that takes as input a batch of data and computes the output of the neural net on each input point given in the batch.\n",
    "\n",
    "The data batch is given as an $n \\times d$ matrix $X$, where each row is a data point.\n",
    "\n",
    "\n",
    "A neural net as considered here requires two sets of weights and biases\n",
    "* The weights that map the input data to the input to hidden units. Call that W_1. The bias weights for this we name $b_1$.\n",
    "\n",
    "* The weights that map the output of the hidden units to the output. Call that W_2. The bias weights for this we name $b_2$.\n",
    "\n",
    "We organize the weighs in matrices $(W_1, W_2)$ and vectors $(b_1,b_2)$ as follows:\n",
    "* The $i'th$ column of $W_1$ are the weights we multiply with the input data to get the input hidden node $i$. The shape of the $W_1$ matrix is $d \\times h$\n",
    "* The bias $b_1$ is a vector of size h, the i'th entry the bias to hidden neuron $i$.\n",
    "* The $i'th$ column of $W_2$ are the weights we multiply with the hidden layer activations to get the input to the i'th output node. $W_2$ is a $h \\times \\textrm{output_size}$ matrix ($h \\times 1$ matrix in our case)\n",
    "* The bias $b_2$ is a vector of size output_size \n",
    "\n",
    "**Task:** In the cell below (partially) complete the neural net class\n",
    "- Implement the predict function of the neural net\n",
    "- Implement the score function (least squares $\\frac{1}{n} \\sum_i (\\textrm{nn}(x_i) - y_i)^2$\n",
    "\n",
    "Tests:\n",
    "- We have a simple test case with random weights. The actual error here is random since we just set random weights.\n",
    "- The second test case uses the weight of a pretreined network for house pricing. Here you should get a score around 0.32 (remove comment to run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net initialized with random values\n",
      "shape of nn_out (10, 1)\n",
      "least squares error:  13.912455240377707\n",
      "data size: 20640 num features: 8\n",
      "Neural net initialized with random values\n",
      "hidden layer size: 7\n",
      "mean house price least squares error: 0.32253261478095707\n",
      "5 house prediction:\n",
      "estimated price , true price\n",
      "[[3.87439488 4.526     ]\n",
      " [3.98254363 3.585     ]\n",
      " [3.81248215 3.521     ]\n",
      " [3.1932362  3.413     ]\n",
      " [2.89667929 3.422     ]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_size=1):\n",
    "        self.W1 = np.random.rand(input_dim, hidden_size)\n",
    "        self.b1 = np.random.rand(1, hidden_size)\n",
    "        self.W2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(1, output_size)\n",
    "        print('Neural net initialized with random values')\n",
    "        \n",
    "    def predict(self, X):    \n",
    "        \"\"\" Evaluate the network on given data batch \n",
    "        \n",
    "        np.maximum may come in handy\n",
    "        \n",
    "        Args:\n",
    "        X: np.array shape (n, d)  Each row is a data point\n",
    "        \n",
    "        Output:\n",
    "        pred: np.array shape (n, 1) output of network on each input point\n",
    "        \"\"\"\n",
    "        # compute the following values\n",
    "        pred = None # the output of neural net n x 1\n",
    "    \n",
    "        ### YOUR CODE HERE\n",
    "        hiddenIn = X @ self.W1 + (np.ones((X.shape[0],1)) @ self.b1) #n x h\n",
    "        nonLinear = np.maximum(hiddenIn,0) #n x h\n",
    "        output = nonLinear @ self.W2 + (np.ones((X.shape[0],1)) @ self.b2) #n x o\n",
    "        ### END CODE\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares loss (1/n sum (nn(x_i) - y_i)^2)\n",
    "        \n",
    "          X: np.array shape (n, d) - Data\n",
    "          y: np.array shape (n, 1) - Targets\n",
    "\n",
    "        \"\"\"\n",
    "        score = None\n",
    "        ### YOUR CODE HERE\n",
    "        pred = self.predict(X)\n",
    "        score = np.mean((pred-y)**2)\n",
    "        ### END CODE\n",
    "        return score\n",
    "        \n",
    "# random data test\n",
    "def simple_test():\n",
    "    input_dim = 3\n",
    "    hidden_size = 8\n",
    "    X = np.random.rand(10, input_dim)\n",
    "    y = np.random.rand(10, 1)\n",
    "    my_net = NN(input_dim=input_dim, hidden_size=hidden_size)\n",
    "\n",
    "    nn_out = my_net.predict(X)\n",
    "    print('shape of nn_out', nn_out.shape) # should be n x 1\n",
    "    print('least squares error: ', my_net.score(X, y))\n",
    "    \n",
    "# actual data test\n",
    "def housing_test():\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from  sklearn.datasets import fetch_california_housing\n",
    "    rdata = fetch_california_housing()\n",
    "    s = StandardScaler()\n",
    "    Xr = rdata.data\n",
    "    yr = rdata.target   \n",
    "    print('data size:', len(yr), 'num features:', Xr.shape[1])\n",
    "    s.fit(Xr)\n",
    "    X_scaled = s.transform(Xr)\n",
    "    house_net = NN(input_dim=Xr.shape[1], hidden_size=8)\n",
    "    weights = np.load('good_weights.npz')\n",
    "    house_net.W1 = weights['W1']\n",
    "    house_net.W2 = weights['W2']\n",
    "    house_net.b1 = weights['b1'].reshape(1, -1)\n",
    "    house_net.b2 = weights['b2'].reshape(1, -1)\n",
    "    print('hidden layer size:', house_net.W1.shape[1])\n",
    "    lsq = house_net.score(X_scaled, yr.reshape(-1, 1))\n",
    "    pred = house_net.predict(X_scaled)\n",
    "    print('mean house price least squares error:', lsq)\n",
    "    print('5 house prediction:\\nestimated price , true price')\n",
    "    print(np.c_[house_net.predict(X_scaled[0:5, :]), yr[0:5]])\n",
    "\n",
    "simple_test()\n",
    "housing_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 3 Install and test Pytorch\n",
    "Later in the course we will be using the deep learning frame work pytorch. So install it. Install torchviz too to help plot computation graphs.\n",
    "(package name is torch i.e. pip3 install torch),.\n",
    "\n",
    "In this exercise your task is to automatic differentation to evaluate derivatives of the sigmoid funcion. \n",
    "Let us use automatic differentation for another gradient descent algorithm.\n",
    "\n",
    "Lets do a similar exercise to last time just with some data and Linear Regression Gradient Descent.\n",
    "First we need to understand that to represent data we must use torch tensors. Tensors are very much like numpy arrays just with some extra functionality.\n",
    "The thing we will consider is the backward function that computes gradients of whatever computation you have made using torch tensors.\n",
    "\n",
    "To see how this works, lets see an example.\n",
    "Lets evaluate the gradient of the sigmoid function without actually knowing the formula. All you need to know is how to compute the function using standard functions i.e. $s(z) = 1/(1+e^{-z})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(z)=1/(1 + e^{-z}) at z = 0 tensor([0.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "sz = 1.0 / (1 + torch.exp(-z))\n",
    "sz.backward() # compute gradient of sz relative to z in this case\n",
    "print('Gradient of f(z)=1/(1 + e^{-z}) at z = 0', z.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3 Continued: Gradient Descent with pytorch\n",
    "In deep learning frameworks all we usually need to do is to show how to compute the cost in a given point and the system then automatically computes the gradient in that data point for you. We will se how later in this course, in this exercise we will try and see if we can use this functionality to do gradient descent.\n",
    "The example will be similar to last weeks gradient descent except now we actually make a data set to run gradient descent on for Linear Regression.\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "We create a data set $D$ that will consist of $n=100$ data points in 2D $(x_1,x_2)$ i.e. two features $x_1, x_2$. \n",
    "The data feature vectors of $x_1$ and $x_2$ are made orthogonal and $x_1$ has unit norm while $x_2$ has norm $a$.\n",
    "\n",
    "We generate a target vector \n",
    "$$\n",
    "y = x_1+x_2\n",
    "$$ \n",
    "which is also a vector of length $n$ i.e. the data we are trying to fit comes from *(a very simple)* linear model.\n",
    "\n",
    "Remember linear regression the in sample error \n",
    "$$\n",
    "\\textrm{E}_\\textrm{in}(w) = \\frac{1}{n} \\sum_{i=1}^n (w^\\intercal x_i - y_i)^2 = \\frac{1}{n} \\|Xw -y\\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "We have written the code to generate data and the surrounding Gradient Descent for loop, all you need is to write the code for computing the cost (ein).\n",
    "You can only use commands from torch here (no numpy), but you can use standard operators like $+,-,*,/,**$ on torch tensors that work like their numpy equivalent and torch.sum may be very handy\n",
    "**Complete the gradient descent code below by computing cost using standard operations and torch commands only**\n",
    "\n",
    "The gradient descent will start the search at $w=(42, 2)$ for some reason. We have also sat an almost arbitrary learning rate. You can change both if you like.\n",
    "\n",
    "\n",
    "To see how this linear regression exercise relates to the gradient descent exercie from last week try increasing the value of $a$. \n",
    "\n",
    "**For the $a$ specified, the lecturer gets an epoch 41 Cost of approximately 0.25 and a w of approximately [5.75, 0.25]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Cost 17.3\n",
      "epoch 1: Cost 15.190625000000002\n",
      "epoch 2: Cost 13.692634062500005\n",
      "epoch 3: Cost 12.356926041406258\n",
      "epoch 4: Cost 11.152098704369148\n",
      "epoch 5: Cost 10.064767998773155\n",
      "epoch 6: Cost 9.083453075615974\n",
      "epoch 7: Cost 8.197816399012343\n",
      "epoch 8: Cost 7.398529300039398\n",
      "epoch 9: Cost 6.6771726932827855\n",
      "epoch 10: Cost 6.026148355687603\n",
      "epoch 11: Cost 5.43859889100806\n",
      "epoch 12: Cost 4.908335499134772\n",
      "epoch 13: Cost 4.429772787969132\n",
      "epoch 14: Cost 3.9978699411421412\n",
      "epoch 15: Cost 3.6080776218807817\n",
      "epoch 16: Cost 3.2562900537474047\n",
      "epoch 17: Cost 2.9388017735070338\n",
      "epoch 18: Cost 2.6522686005900984\n",
      "epoch 19: Cost 2.393672412032563\n",
      "epoch 20: Cost 2.1602893518593884\n",
      "epoch 21: Cost 1.9496611400530979\n",
      "epoch 22: Cost 1.7595691788979213\n",
      "epoch 23: Cost 1.588011183955374\n",
      "epoch 24: Cost 1.4331800935197254\n",
      "epoch 25: Cost 1.293445034401552\n",
      "epoch 26: Cost 1.1673341435474007\n",
      "epoch 27: Cost 1.053519064551529\n",
      "epoch 28: Cost 0.9508009557577551\n",
      "epoch 29: Cost 0.8580978625713738\n",
      "epoch 30: Cost 0.7744333209706651\n",
      "epoch 31: Cost 0.6989260721760251\n",
      "epoch 32: Cost 0.6307807801388625\n",
      "epoch 33: Cost 0.5692796540753234\n",
      "epoch 34: Cost 0.5137748878029793\n",
      "epoch 35: Cost 0.4636818362421889\n",
      "epoch 36: Cost 0.41847285720857536\n",
      "epoch 37: Cost 0.37767175363073924\n",
      "epoch 38: Cost 0.34084875765174216\n",
      "epoch 39: Cost 0.3076160037806973\n",
      "epoch 40: Cost 0.27762344341207934\n",
      "epoch 41: Cost 0.25055515767940156\n",
      "best w found tensor([5.7553, 0.2500], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch # the torch tensor library\n",
    "\n",
    "## CREATE SOME DATA\n",
    "n = 100\n",
    "x1 = np.random.rand(n)\n",
    "x2 = np.random.rand(n)\n",
    "## Grahm schmidt process - ignore\n",
    "x1 = x1/np.linalg.norm(x1)\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "x2 = x2 - np.dot(x1, x2) * x1 #\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "\n",
    "## CREATE THE DATA MATRIX\n",
    "a = 4.0\n",
    "D = np.c_[x1, a*x2]\n",
    "# CREATE TARGET FUNCTION VECTOR\n",
    "y = x1 + x2\n",
    "\n",
    "# MAKE TORCH VARIABLES TO USE\n",
    "X = torch.from_numpy(D).double()\n",
    "ty = torch.from_numpy(y).double()\n",
    "ni = torch.tensor(1./n, dtype=torch.double)\n",
    "\n",
    "def torch_gd():\n",
    "    w = torch.tensor([42.0, 2.0], dtype=torch.double)\n",
    "    lr = torch.tensor(10.0/a, dtype=torch.double)\n",
    "    epochs = 42\n",
    "    cost_hist = []\n",
    "    for i in range(epochs):\n",
    "        w.requires_grad_() # say i want gradient relative to w in upcoming computation\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE - Compute Ein for linear regression as function of w and store in cost 1 - 4 lines\n",
    "        cost =  ni * torch.sum((X @ w - ty)**2)\n",
    "        ### END CODE\n",
    "        cost_hist.append(cost)\n",
    "        print('epoch {0}: Cost {1}'.format(i, cost))\n",
    "        cost.backward() # compute gradient cost as function of w\n",
    "        w = w - lr * w.grad # update w\n",
    "        w.detach_() # removes w from current computation graph\n",
    "    print('best w found', w)\n",
    "torch_gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Backpropagation on small example, manually\n",
    "\n",
    "Consider the simple neural net \n",
    "$$\n",
    "\\textrm{nn}(x_1,x_2) = w_3 \\cdot \\textrm{relu}(w_1 \\cdot x_1 + w_2 \\cdot x_2)\n",
    "$$\n",
    "where $\\textrm{relu}(x) = \\max(x,0)$ and $w_1, w_2, w_3$ are the weights of the network\n",
    "\n",
    "We only have one input point $x=(x_1,x_2) = (3.0, 1.0)$ with target value $y=9.0$\n",
    "\n",
    "Consider the least squares error \n",
    "$$\n",
    "e=(y-\\textrm{nn}(x))^2\n",
    "$$\n",
    "\n",
    "We need to minimize the error over $w_1, w_2, w_3$ and will do that using the gradients of $e$ relative to $w_1, w_2, w_3$.\n",
    "Initialize $w_1=1,w_2=2, w_3=1$\n",
    "\n",
    "Draw the computational graph for $e$ and run the forward pass to compute the output of the neural network on $x$ and the least squares error, and then run the backwards pass to compute the partial derivative of the loss relative to $w_1,w_2, w_3$ on the fixed input $x,y$ with the fixed values for $w_1,w_2,w_3$.\n",
    "\n",
    "Write the python code that performs the forward and backwards pass below and evaluate the cost and the gradient\n",
    "using notation similar to\n",
    "http://cs231n.github.io/optimization-2/\n",
    "\n",
    "Print intermediate steps in both the forward and the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the python forward and backward pass here\n",
      "z1 3.0\n",
      "z2 2.0\n",
      "hin 5.0\n",
      "hout 5.0\n",
      "pred 5.0\n",
      "diff -4.0\n",
      "ls error 16.0\n",
      "dz 3.0 1.0\n",
      "d_diff -8.0\n",
      "d_pred -8.0\n",
      "d_hout -8.0\n",
      "d_hin -8.0\n",
      "d_w1: -24.0\n",
      "d_w2: -8.0\n",
      "d_w3: -40.0\n"
     ]
    }
   ],
   "source": [
    "print('Do the python forward and backward pass here')\n",
    "x1 = 3.0\n",
    "x2 = 1.0 \n",
    "y = 9.\n",
    "w1 = 1.0\n",
    "w2 = 2.0\n",
    "w3 = 1.0\n",
    "### YOUR CODE HERE\n",
    "# Long Forward Pass\n",
    "z1 = w1 * x1\n",
    "z2 = w2 * x2\n",
    "hin = z1 + z2\n",
    "hout = max(hin, 0)\n",
    "pred = w3 * hout\n",
    "diff = pred - y\n",
    "e = diff**2\n",
    "print('z1', z1)\n",
    "print('z2', z2)\n",
    "print('hin', hin)\n",
    "print('hout', hout)\n",
    "print('pred', pred)\n",
    "print('diff', diff)\n",
    "print('ls error', e)\n",
    "\n",
    "# backwards pass\n",
    "de_diff = 2 * diff\n",
    "ddiff_pred = 1\n",
    "de_pred = de_diff * ddiff_pred\n",
    "dpred_w3 = hout\n",
    "de_w3 = de_pred * dpred_w3 # dout_w3 = rs\n",
    "dpred_hout = w3\n",
    "de_hout = de_pred * dpred_hout\n",
    "dhout_hin = hin > 0\n",
    "de_hin = de_hout * dhout_hin\n",
    "dhin_z1 = 1\n",
    "dhin_z2 = 1\n",
    "de_z1 = de_hin * dhin_z1\n",
    "de_z2 = de_hin * dhin_z2\n",
    "dz1_w1 = x1\n",
    "dz2_w2 = x2\n",
    "print('dz', dz1_w1, dz2_w2)\n",
    "de_w1 = de_z1 * dz1_w1\n",
    "de_w2 = de_z2 * dz2_w2\n",
    "print('d_diff', de_diff)\n",
    "print('d_pred', de_diff)\n",
    "print('d_hout', de_diff)\n",
    "print('d_hin', de_diff)\n",
    "print('d_w1:', de_w1)\n",
    "print('d_w2:', de_w2)\n",
    "print('d_w3:', de_w3)\n",
    "\n",
    "### END CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Automated Backpropagation using pytorch\n",
    "In this exercise we will check our results from the previous exercise using pytorch.\n",
    "\n",
    "For this we only need to code the forward pass and let automatic differentation take care of the rest!\n",
    "\n",
    "**Task:** Write the forward pass in the cell below and use automatic differentation to test your answer from above.\n",
    "\n",
    "Use x.retain_grad() to keep the gradient of any intermediate computation used in the forward pass to compare with above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass\n",
      "least squares error tensor([[16.]], grad_fn=<PowBackward0>)\n",
      "d_diff tensor([[-8.]])\n",
      "d_pred tensor([[-8.]])\n",
      "d_hout tensor([[-8.]])\n",
      "d_hin tensor([[-8.]])\n",
      "d_w1 tensor([[-24.]])\n",
      "d_w2 tensor([[-8.]])\n",
      "d_w3 tensor([[-40.]])\n",
      "Lets show the computation graph\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n -->\n<!-- Pages: 1 -->\n<svg width=\"223pt\" height=\"397pt\"\n viewBox=\"0.00 0.00 222.50 397.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 393)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-393 218.5,-393 218.5,4 -4,4\"/>\n<!-- 4705104320 -->\n<g id=\"node1\" class=\"node\">\n<title>4705104320</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"187,-21 94,-21 94,0 187,0 187,-21\"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">PowBackward0</text>\n</g>\n<!-- 4705104800 -->\n<g id=\"node2\" class=\"node\">\n<title>4705104800</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"185.5,-78 95.5,-78 95.5,-57 185.5,-57 185.5,-78\"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">SubBackward0</text>\n</g>\n<!-- 4705104800&#45;&gt;4705104320 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4705104800&#45;&gt;4705104320</title>\n<path fill=\"none\" stroke=\"black\" d=\"M140.5,-56.92C140.5,-49.91 140.5,-40.14 140.5,-31.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"144,-31.34 140.5,-21.34 137,-31.34 144,-31.34\"/>\n</g>\n<!-- 4705105232 -->\n<g id=\"node3\" class=\"node\">\n<title>4705105232</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"186,-135 95,-135 95,-114 186,-114 186,-135\"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n</g>\n<!-- 4705105232&#45;&gt;4705104800 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4705105232&#45;&gt;4705104800</title>\n<path fill=\"none\" stroke=\"black\" d=\"M140.5,-113.92C140.5,-106.91 140.5,-97.14 140.5,-88.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"144,-88.34 140.5,-78.34 137,-88.34 144,-88.34\"/>\n</g>\n<!-- 4705105376 -->\n<g id=\"node4\" class=\"node\">\n<title>4705105376</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"142,-198.5 45,-198.5 45,-177.5 142,-177.5 142,-198.5\"/>\n<text text-anchor=\"middle\" x=\"93.5\" y=\"-184.9\" font-family=\"Times,serif\" font-size=\"12.00\">ClampBackward</text>\n</g>\n<!-- 4705105376&#45;&gt;4705105232 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4705105376&#45;&gt;4705105232</title>\n<path fill=\"none\" stroke=\"black\" d=\"M100.84,-177.39C107.77,-168.33 118.33,-154.51 126.81,-143.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"129.76,-145.32 133.05,-135.24 124.2,-141.07 129.76,-145.32\"/>\n</g>\n<!-- 4705105520 -->\n<g id=\"node5\" class=\"node\">\n<title>4705105520</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139.5,-262 47.5,-262 47.5,-241 139.5,-241 139.5,-262\"/>\n<text text-anchor=\"middle\" x=\"93.5\" y=\"-248.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n</g>\n<!-- 4705105520&#45;&gt;4705105376 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4705105520&#45;&gt;4705105376</title>\n<path fill=\"none\" stroke=\"black\" d=\"M93.5,-240.89C93.5,-232.37 93.5,-219.64 93.5,-208.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"97,-208.74 93.5,-198.74 90,-208.74 97,-208.74\"/>\n</g>\n<!-- 4705105616 -->\n<g id=\"node6\" class=\"node\">\n<title>4705105616</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"85,-319 0,-319 0,-298 85,-298 85,-319\"/>\n<text text-anchor=\"middle\" x=\"42.5\" y=\"-305.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n</g>\n<!-- 4705105616&#45;&gt;4705105520 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4705105616&#45;&gt;4705105520</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51.38,-297.92C58.54,-290.21 68.78,-279.16 77.37,-269.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"80.14,-272.05 84.37,-262.34 75.01,-267.29 80.14,-272.05\"/>\n</g>\n<!-- 4705105760 -->\n<g id=\"node7\" class=\"node\">\n<title>4705105760</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"69.5,-389 15.5,-389 15.5,-355 69.5,-355 69.5,-389\"/>\n<text text-anchor=\"middle\" x=\"42.5\" y=\"-375.4\" font-family=\"Times,serif\" font-size=\"12.00\">W1</text>\n<text text-anchor=\"middle\" x=\"42.5\" y=\"-362.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 1)</text>\n</g>\n<!-- 4705105760&#45;&gt;4705105616 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4705105760&#45;&gt;4705105616</title>\n<path fill=\"none\" stroke=\"black\" d=\"M42.5,-354.84C42.5,-347.01 42.5,-337.54 42.5,-329.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"46,-329.04 42.5,-319.04 39,-329.04 46,-329.04\"/>\n</g>\n<!-- 4705105664 -->\n<g id=\"node8\" class=\"node\">\n<title>4705105664</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"188,-319 103,-319 103,-298 188,-298 188,-319\"/>\n<text text-anchor=\"middle\" x=\"145.5\" y=\"-305.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n</g>\n<!-- 4705105664&#45;&gt;4705105520 -->\n<g id=\"edge7\" class=\"edge\">\n<title>4705105664&#45;&gt;4705105520</title>\n<path fill=\"none\" stroke=\"black\" d=\"M136.44,-297.92C129.08,-290.13 118.5,-278.94 109.68,-269.62\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"112.22,-267.2 102.8,-262.34 107.13,-272.01 112.22,-267.2\"/>\n</g>\n<!-- 4705105808 -->\n<g id=\"node9\" class=\"node\">\n<title>4705105808</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"172.5,-389 118.5,-389 118.5,-355 172.5,-355 172.5,-389\"/>\n<text text-anchor=\"middle\" x=\"145.5\" y=\"-375.4\" font-family=\"Times,serif\" font-size=\"12.00\">W2</text>\n<text text-anchor=\"middle\" x=\"145.5\" y=\"-362.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 1)</text>\n</g>\n<!-- 4705105808&#45;&gt;4705105664 -->\n<g id=\"edge8\" class=\"edge\">\n<title>4705105808&#45;&gt;4705105664</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.5,-354.84C145.5,-347.01 145.5,-337.54 145.5,-329.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"149,-329.04 145.5,-319.04 142,-329.04 149,-329.04\"/>\n</g>\n<!-- 4705105424 -->\n<g id=\"node10\" class=\"node\">\n<title>4705105424</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"214.5,-205 160.5,-205 160.5,-171 214.5,-171 214.5,-205\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\">W3</text>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 1)</text>\n</g>\n<!-- 4705105424&#45;&gt;4705105232 -->\n<g id=\"edge9\" class=\"edge\">\n<title>4705105424&#45;&gt;4705105232</title>\n<path fill=\"none\" stroke=\"black\" d=\"M175.15,-170.84C168.62,-162.3 160.6,-151.8 153.92,-143.06\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"156.65,-140.86 147.79,-135.04 151.08,-145.11 156.65,-140.86\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.dot.Digraph at 0x118723280>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot # install this package \n",
    "\n",
    "x1 = torch.tensor([[3.]])\n",
    "x2 = torch.tensor([[1.]])\n",
    "y = torch.tensor([9.])\n",
    "W1 = torch.tensor([[1.]], requires_grad=True)\n",
    "W2 = torch.tensor([[2.]], requires_grad=True)\n",
    "W3 = torch.tensor([[1.]], requires_grad=True)\n",
    "### YOUR CODE HERE - The clamp function may be usefull\n",
    "hin = x1 @ W1 + x2 @ W2\n",
    "hout = hin.clamp(min=0)\n",
    "pred = hout * W3\n",
    "diff = pred - y\n",
    "loss = diff**2\n",
    "print('forward pass')\n",
    "print('least squares error', loss)\n",
    "hin.retain_grad()\n",
    "hout.retain_grad()\n",
    "pred.retain_grad()\n",
    "diff.retain_grad()\n",
    "loss.backward()\n",
    "print('d_diff', diff.grad)\n",
    "print('d_pred', pred.grad)\n",
    "print('d_hout', hout.grad)\n",
    "print('d_hin', hin.grad)\n",
    "\n",
    "### END CODE\n",
    "# print the graph - change naming appropriately\n",
    "print('d_w1', W1.grad)\n",
    "print('d_w2', W2.grad)\n",
    "print('d_w3', W3.grad)\n",
    "print('Lets show the computation graph')\n",
    "make_dot(loss, params={'W1': W1, 'W2': W2, 'W3': W3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 6: Matrix of Derivatives for applying function elementwise to vector \n",
    "In this exercise we consider the matrix of derivatives when mapping a vector of size $n$ to a vector of size $n$ by applying a function $f$ to each entry in the input.\n",
    "\n",
    "Let $f$ be a smooth function from $\\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Let $F: \\mathbb{R}^d \\rightarrow \\mathbb{R^d}$ defined as $F(v) = \\left[f(v_1), \\dots, f(v_d)\\right]$\n",
    "\n",
    "- Show that the Matrix of Derivative $\\frac{\\partial F}{\\partial x}= D_f$, the $d \\times d$ diagonal matrix where the $i$'th diagonal entry is $f'(v_i)$.\n",
    "- If $f(x) = 1/(1+e^{-x})$ i.e. the logistic sigmoid function then how does the Diagonal matrix look\n",
    "- If $f(x) = \\max(0, x)$ i.e. relu how does the Diagonal Matrix look\n",
    "- Consider a neural net backpropagation step and let hout be the output of applying F to the vector hin and assume we have computed the vector $\\frac{\\partial L}{\\partial \\textrm{hout}}$ ($1 \\times h$)\n",
    "    and wish to compute  $\\frac{\\partial L}{\\partial \\textrm{hin}}$ (also $1\\times h)$. Write the python code that achieves this below.\n",
    "    \n",
    "## SOLUTION MATH HERE\n",
    " The matrix of derivatives is a $d \\times d$ matrix\n",
    " \n",
    " The derivative of $f_i = f(v_i)$ after $v_j$ is zero if $i != j$. So we get a diagonal matrix.\n",
    " \n",
    " If i==j then we need the derivative of $f(v_i)$ as a function of v_i which is $f'(v_i)$.\n",
    " \n",
    " For sigmoid we get $D_{i,i} = f(x_i)(1-f(x_i))$ \n",
    " \n",
    " For relu we get $D_{i, i} = x_i > 0$ (indicator variables)\n",
    "## END SOLUTION\n",
    "\n",
    "**hint:** There may be a more efficient way than actually creating the diagonal matrix and multiplying on the backpropagated derivative but you do not have to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1, 3) (1, 3)\n",
      "d_hin relu: [[0 2 3]]\n",
      "d_hin sigmoid: [[0.19661193 0.20998717 0.05298812]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "h_in = np.array([[-1, 2, 4]])\n",
    "d_hout = np.array([[1,2,3]])\n",
    "print('shapes:', h_in.shape, d_hout.shape)\n",
    "def relu_grad(d_hout, hin):\n",
    "    d_hin = None\n",
    "    ### YOUR CODE HERE\n",
    "    d_hin = d_hout.copy()\n",
    "    d_hin[hin<0] = 0\n",
    "    #d_hin = d_hin * (hin<0)\n",
    "    \n",
    "    ### END CODE\n",
    "    return d_hin\n",
    "\n",
    "def sigmoid_grad(d_hout, hin):\n",
    "    d_hin = None\n",
    "    ### YOUR CODE HERE\n",
    "    def sigmoid(x):\n",
    "        return 1./(1+np.exp(-x))\n",
    "    d_hin =  d_hout * (sigmoid(hin)*(1-sigmoid(hin))) # entrywise multiplication\n",
    "    ### END CODE\n",
    "    return d_hin\n",
    "\n",
    "print('d_hin relu:', relu_grad(d_hout, h_in))\n",
    "# should be [0, 2, 3]\n",
    "print('d_hin sigmoid:', sigmoid_grad(d_hout, h_in))\n",
    "# should be ~ [0.196..., 0.209..., 0.052...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 7: Neural Nets by automatic differentation in pytorch\n",
    "Before we introduce our selves to the idiomatic way of writing neural net code in pytorch we will implement a pair of basic basic neural nets and train it using just automatic differentation and the optim module from pytorch that implements different gradient based optimization methods.\n",
    "\n",
    "**The task is:**\n",
    "\n",
    "Using pytorch implement Linear Regression with a weight decay.\n",
    "(named Ridge Regression in litterature ) using Gradient Descent as Learning Algorithm \n",
    "and implement a one hidden layer neural net with relu activation for regression (identity output neuron) with with the same cost\n",
    "\n",
    "Let $w, b$ be the parameters of the function $f$ we are trying to learn, then least squares with weight decay cost for weight decay parameter $c$ is\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n (f(x)-y)^2 + c \\sum_{i=1}^d w_i^2\n",
    "$$\n",
    "Note we do not penalize the bias weights.\n",
    "\n",
    "1. Implement Linear Regression with Gradient Descent and test on the Boston data set for house prices (see cell below)\n",
    "   Linear regression is simply a neural net with no hidden layer and one output neuron. We have implemented the fit methods to show you how to use the optim module.\n",
    "   That means all you need to do is implement the cost method.\n",
    "2. Implement  implement a standard one hidden layer neural net for regression. That means identity output activation and least squares error with weight decay. The formula for weight decay is the same, i.e. you need to add the squared weight to the error for all weights in all layers. \n",
    "For nonlineariry use relu. See cell two below\n",
    "   In this exercise you need to complete the following two methods (hint: fit is similar to linear regression fit(\n",
    "   - cost - compute the least squares cost of the network on data and return the pytorch tensor of that\n",
    "   - fit - train 100 steps og gradient descent using optim package - find a good learning rate your self\n",
    "\n",
    "We test your implementation on a standard regression data set for house prices and compare to the sklearn build in Ridge Regression. \n",
    "We only consider in sample error, which is of course not what we care about in the real world!.\n",
    "\n",
    "Your Linear Regression implementation should get close to the sklearn Ridge Regression implementation that we have included\n",
    "\n",
    "For the Neural Net you should do better (in sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Load and Prepare Data *****\n",
      "\n",
      " ***** Test Sklearn Ridge Regression for Comparison *****\n",
      "Ridge Regression Score: 21.89484192522429\n",
      "\n",
      " ***** Make data to torch tensors *****\n",
      "\n",
      " ***** Run Torch Linear Regression Gradient Descent *****\n",
      "epoch: 0 least squares (regularized loss) 592.1469116210938\n",
      "epoch: 10 least squares (regularized loss) 32.51519012451172\n",
      "epoch: 20 least squares (regularized loss) 26.201087951660156\n",
      "epoch: 30 least squares (regularized loss) 25.99349021911621\n",
      "epoch: 40 least squares (regularized loss) 25.94190216064453\n",
      "epoch: 50 least squares (regularized loss) 25.922088623046875\n",
      "epoch: 60 least squares (regularized loss) 25.91387367248535\n",
      "epoch: 70 least squares (regularized loss) 25.910276412963867\n",
      "epoch: 80 least squares (regularized loss) 25.90863037109375\n",
      "epoch: 90 least squares (regularized loss) 25.90785026550293\n",
      "pytorch Linear Regression Regression least squares score: 22.676130294799805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "\n",
    "print('*'*5, 'Load and Prepare Data', '*'*5)\n",
    "dataset = load_boston()\n",
    "# print('dataset', dataset)\n",
    "X, y = dataset.data, dataset.target\n",
    "X = (X - X.mean(axis=0))/(X.std(axis=0))\n",
    "#print('data stats', X.shape, X.mean(axis=0), X.std(axis=0))\n",
    "ridge=linear_model.Ridge(alpha=0.1, fit_intercept=True)\n",
    "ridge.fit(X, y)\n",
    "# print(ridge.coef_, ridge.intercept_)\n",
    "print('\\n', '*'*5, 'Test Sklearn Ridge Regression for Comparison', '*'*5)\n",
    "print('Ridge Regression Score:', ((ridge.predict(X)-y)**2).mean())\n",
    "\n",
    "print('\\n', '*'*5, 'Make data to torch tensors', '*'*5)\n",
    "tX = torch.from_numpy(X).float()\n",
    "ty = torch.from_numpy(y).float().view(-1, 1)\n",
    "\n",
    "\n",
    "class LR():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def cost(self, X, y, w, b, c=0):\n",
    "        \"\"\" Compute Regularized Least Squares Loss\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          y: torch.tensor shape (n, 1) - Targets\n",
    "          w: torch.tensor shape (d, 1) - weights\n",
    "          b: torch.tensor shape (1, 1) - bias weight\n",
    "          c: scalar, weight decay parameter \n",
    "          \n",
    "          returns (regularized) cost tensor        \n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        pred = X @ w + b \n",
    "        loss = torch.mean((pred-y)**2) \n",
    "        reg_loss = torch.sum(w**2)\n",
    "        ### END CODE\n",
    "        return loss + c * reg_loss\n",
    "    \n",
    "    def fit(self, X, y, c=0):\n",
    "        \"\"\" GD Learning Algorithm for Ridge Regression with pytorch\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         c: ridge regression weight decay parameter (lambda)\n",
    "        \"\"\"\n",
    "        w = torch.zeros(X.shape[1], 1, requires_grad=True)\n",
    "        b = torch.zeros(1, 1, requires_grad=True)\n",
    "        sgd = optim.SGD(params={w, b}, lr=0.1)\n",
    "        for i in range(100):\n",
    "            sgd.zero_grad()\n",
    "            loss = self.cost(X, y, w, b, c=c)\n",
    "            if i % 10 == 0:\n",
    "                print('epoch:', i, 'least squares (regularized loss)', loss.item())\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        self.w = w.clone()\n",
    "        self.b = b.clone()\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares cost for model \n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         \n",
    "        returns least squares score of model on data X with targets y\n",
    "        \"\"\"\n",
    "        score = self.cost(X, y, self.w, self.b, c=0)\n",
    "        return score\n",
    "\n",
    "print('\\n', '*'*5, 'Run Torch Linear Regression Gradient Descent', '*'*5)\n",
    "\n",
    "tlr = LR()\n",
    "tlr.fit(tX, ty, 0.1)\n",
    "print('pytorch Linear Regression Regression least squares score:', tlr.score(tX, ty).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 nn least squares loss 642.3568725585938\n",
      "epoch: 10 nn least squares loss 27.601709365844727\n",
      "epoch: 20 nn least squares loss 21.81852912902832\n",
      "epoch: 30 nn least squares loss 19.55316162109375\n",
      "epoch: 40 nn least squares loss 18.162565231323242\n",
      "epoch: 50 nn least squares loss 17.190034866333008\n",
      "epoch: 60 nn least squares loss 16.485641479492188\n",
      "epoch: 70 nn least squares loss 15.916590690612793\n",
      "epoch: 80 nn least squares loss 15.467313766479492\n",
      "epoch: 90 nn least squares loss 15.086695671081543\n",
      "pytorch Neural Net Regression least squares score: 12.451496124267578\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "\n",
    "dataset = load_boston()\n",
    "X, y = dataset.data, dataset.target\n",
    "X = (X - X.mean(axis=0))/(X.std(axis=0))\n",
    "tX = torch.from_numpy(X).float()\n",
    "ty = torch.from_numpy(y).float().view(-1, 1)\n",
    "\n",
    "class NN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def cost(self, X, y, W1, b1, W2, b2, c=0):\n",
    "        \"\"\" Compute (Regularized) Least Squares Loss of neural net\n",
    "        The clamp function may be usefull\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          y: torch.tensor shape (n, 1) - Targets\n",
    "          W1: torch.tensor shape (d, h) - weights\n",
    "          b1: torch.tensor shape (1, h) - bias weight\n",
    "          W2: torch.tensor shape (h, 1) - weights\n",
    "          b2: torch.tensor shape (1, 1) - bias weight\n",
    "          c: ridge regression weight decay parameter \n",
    "    \n",
    "        returns (weight decay) cost tensor\n",
    "        \"\"\"\n",
    "   \n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        hin = X @ W1 + b1\n",
    "        hout = hin.clamp(min=0)\n",
    "        pred = hout @ W2 + b2\n",
    "        loss = torch.mean((pred-y)**2) \n",
    "        reg_loss = c * torch.sum(W1**2) + c * torch.sum(W2**2)\n",
    "        loss = loss + reg_loss\n",
    "        ### END CODE\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, hidden_size=32, c=0.01):   \n",
    "        \"\"\" GD Learning Algorithm for Ridge Regression with pytorch\n",
    "        \n",
    "         Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         hidden_size: int \n",
    "         c: float weight decay parameter (lambda)\n",
    "        \"\"\"\n",
    "        input_dim = X.shape[1]        \n",
    "        W1 = torch.randn(input_dim, hidden_size, requires_grad=True)\n",
    "        b1 = torch.randn(1, hidden_size, requires_grad=True)\n",
    "        W2 = torch.randn(hidden_size, 1, requires_grad=True)\n",
    "        b2 = torch.randn(1, 1, requires_grad=True)\n",
    "        ### YOUR CODE HERE\n",
    "        sgd = optim.SGD(params={W1, W2, b1, b2}, lr=0.01)\n",
    "        for i in range(100):\n",
    "            sgd.zero_grad()\n",
    "            loss = self.cost(X, y, W1, b1, W2, b2, c=c)\n",
    "            if i % 10 == 0:\n",
    "                print('epoch:', i, 'nn least squares loss', loss.item())\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        ### END CODE\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares cost for model \n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         \n",
    "        returns least squares score of model on data X with targets y\n",
    "        \"\"\"\n",
    "        score = self.cost(X, y, self.W1, self.b1, self.W2, self.b2, c=0)\n",
    "        return score\n",
    "\n",
    "\n",
    "net = NN()\n",
    "net.fit(tX, ty, hidden_size=16, c=0.01)\n",
    "print('pytorch Neural Net Regression least squares score:', net.score(tX, ty).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1c4ada40c26a7bd7c0630a7e50cf5a02a3f4dae7aa994c6307f04e05b2a8a1d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
